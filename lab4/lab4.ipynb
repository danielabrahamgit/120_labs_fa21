{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# EE 120 Lab 5: Deconvolution\n",
    "\n",
    "**Signals and Systems** at UC Berkeley\n",
    "\n",
    "Acknowledgements:\n",
    "\n",
    "- **Spring 2019** (v1.0): Dominic Carrano, Ilya Chugunov, Babak Ayazifar  \n",
    "- **Fall 2019** (v2.0): Dominic Carrano, Ilya Chugunov  \n",
    "- **Spring 2020** (v2.1): Dominic Carrano, Ilya Chugunov\n",
    "- **Spring 2021** (v2.2): Jingjia Chen\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import IPython.display as ipd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import scipy.signal\n",
    "from scipy import signal, linalg\n",
    "from scipy.io import wavfile\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Background\n",
    "\n",
    "Note: For simplicity, we'll consider all signals and systems in this lab as discrete-time entities, since that's what computers use. In truth, there are other steps involved, such as sampling and quantization. In class, we denote discrete time signal with square bracket $x[n]$, in this lab, since everything will be in discrete time regime, $x(n)$ also represents discrete time signal, and $X(\\omega)$ is to represent the corresponding DTFT."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Motivation: Signal Restoration\n",
    "\n",
    "*Degradation* (also referred to as *distortion*) is a common problem in signal and image processing: you want access to a \"ground truth\" signal $x(n)$, but due to some real-world physical constraints, you can only observe $y(n)$, a corrupted version of $x(n)$. \n",
    "\n",
    "For example, suppose you're playing a song $x(n)$ in a large concert hall. Due to the acoustics of the room, your audience hears $y(n)$, the song superimposed with a quieter echo of it (that is, $x(n)$ plus a delayed and attenuated copy of $x(n)$).\n",
    "\n",
    "In this lab we address how to compensate for signal distortions. Our goal is to produce $\\hat{x}(n)$, an estimate of the original signal $x(n)$, using the measured signal $y(n)$. The process of determining $\\hat{x}(n)$ using $y(n)$ is called *signal restoration*."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Problem Formulation\n",
    "\n",
    "From a block-diagram perspective, we can think of this *degradation* as a *system* $H$, which outputs $y(n)$, our measured signal, from input $x(n)$, the true signal. \n",
    "\n",
    "<img src=\"figs/degrade.png\" width=\"700px\"></img>\n",
    "\n",
    "Our job is to design a *restoration system*, $R$, that attempts to undo the effects of $H$ to give us $\\hat{x}(n)$, a nice approximation of $x(n)$. Using the concert hall example from above, $H$ could be the acoustics of the room, and $R$ could be a specialized filter chip in a de-echoing concert microphone you bought from your local microphone dealer."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Designing $R$\n",
    "\n",
    "In general, $H$ could be *any* system (e.g., nonlinear, time-varying, etc.), and undoing it may be intractable. To simplify things, we'll assume: \n",
    "- $H$, the degradation system, \n",
    "is DT-LTI (with frequency response $H(\\omega)$ and impulse response $h(n)$).\n",
    "- $R$, the recovery system we choose, is DT-LTI (with frequency response $R(\\omega)$ and impulse response $r(n)$).\n",
    "\n",
    "In many scenarios of practical interest (e.g. acoustic echoes, system lag, image blur), we often can justify an approximation a real-life system as LTI. \n",
    "\n",
    "Since $H$ is LTI, we can describe it's behaviour as a convolution, $y(n) = (x * h)(n)$. We want to design $R$ to *undo* this convolution, performing what's called ***deconvolution***, in the sense that we want to **recover** $x(n)$. For this problem it's easier to work in the frequency domain, where our time-signal convolution just becomes frequency-response multiplication:\n",
    "\n",
    "$$Y(\\omega) = H(\\omega) X(\\omega),$$\n",
    "\n",
    "and\n",
    "\n",
    "$$\\hat{X}(\\omega) = R(\\omega) Y(\\omega) = R(\\omega) H(\\omega) X(\\omega),$$\n",
    "\n",
    "Algebraically, if we pick $R(\\omega) = 1 / H(\\omega)$, this will five us $\\hat{X}(\\omega) = X(\\omega)$, from which we can take the inverse DTFT to recover the original signal. Effectively, we compute $\\hat{X}(\\omega) = Y(\\omega) / H(\\omega)$.\n",
    "\n",
    "This algorithm is known as *Fourier deconvolution* (sometimes also called \"inverse filtering\" or \"direct deconvolution\"), since we perform the deconvolution directly based on the Fourier Transform(s) of the systems involved."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Issues and Alternatives\n",
    "\n",
    "Fourier deconvolution computes the multiplicative inverse of $H$'s frequency response, and uses it to characterize the inverse system $R$. \n",
    "\n",
    "This approach has two main issues:\n",
    "1. $H$ may be zero at some frequency $\\omega_0$, in which case the division is not well-defined. Intuitively, all content at $\\omega_0$ is killed, and we're left with no information about it, so we can't invert the behavior.\n",
    "2. Even if $H$ is nonzero over all frequencies, it's often very small, and so $1 / H(\\omega)$ will be huge, and amplify noise that will be present in practical setups.\n",
    "\n",
    "Due to these issues, it's more common to use *Wiener filtering* in practice for deconvolution. Wiener filtering is a more sophisticated technique that uses statistical properties of the signals and noise involved to produce better results. However, Wiener filtering and Fourier deconvolution are similar in spirit (and we don't want to go down a rabbit hole into statistical signal processing), so we'll be using Fourier in this lab.\n",
    "\n",
    "Later in the lab, we'll explore these issues and how they can affect our signals."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Q1: Echo Cancellation\n",
    "\n",
    "A classic problem in signals and systems, acoustics, and digital communications is *echo cancellation*. A sender transmits a signal to someone, and they receive it, along with a delayed, attenuated copy of it. \n",
    "\n",
    "There are many causes of this phenomenon which you can read about in the references, including:\n",
    "- Signal back reflections due to impedance mismatches in electronic circuits (see references 1, 2).\n",
    "- Audio feedback in microphones (see reference 3).\n",
    "- Acoustic properties of the space the signal is being transmitted in. For example, if you send a signal indoors, it may go in multiple directions, with part of the signal going straight to a receiver, and the rest of it bouncing off of several walls before it arrives as a delayed and attenuated copy of the first."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Modelling an echo\n",
    "\n",
    "Many different models for echoes have been considered, and you can find a more comprehensive treatment of this subject in references 4 and 5. Here, we'll consider one of the simpler models for an echo, where our communication channel (the degradation system) is an LTI system with impulse response\n",
    "\n",
    "$$h(n) = \\delta(n) + \\alpha \\delta(n - k)$$\n",
    "\n",
    "where $0 < \\alpha < 1$ is the attenuation of the echo and $k > 0$ is the integer delay of the echo. Assuming $0 < \\alpha < 1$ means the copy is weaker than the original, and $k>0$ that the copy shows up after the original.\n",
    "\n",
    "We can think of this as a channel that transmits the signal perfectly and instantaneously, and also with a $k$-step delay and some attenuation along an echo path. In this problem, we'll send some audio over this channel, and try to undo the corruptions it introduces using deconvolution."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## A Brief Intro to Audio Signals\n",
    "\n",
    "In this question, we'll use [\"Take on Me\" by a-ha](https://www.youtube.com/watch?v=djV11Xbc914) as our test signal. We have provided it for you as a [.wav](https://en.wikipedia.org/wiki/WAV) (pronounced \"wave\") file. Run the cell below and have a listen! It's normal if the cell takes a few seconds to load. You'll see a graphic display pop up with a play button once it's finished loading."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "ipd.Audio(\"wavs/TakeOnMe.wav\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we'll read the file as a numpy array using scipy's WAV file API. The `read` function returns two things: the sampling rate of the audio, and the signal itself."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "# In signal processing code, \"fs\" is conventionally used for sampling frequency in Hertz (cycles/second)\n",
    "fs, data = wavfile.read(\"wavs/TakeOnMe.wav\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Typically, digital audio is sampled at 44.1 kHz, or 44100 Hz, although some more modern formats use 48 kHz. This is motivated by the fact that the human ear can only hear up to ~20 kHz. Given this, the Nyquist criterion suggests that the sampling rate should be at least 40 kHz, so some extra wiggle room is added on. We can easily verify that we're dealing with a sampling rate of 44.1 kHz."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "print(fs) # sampling rate in Hz"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When dealing with real world data, a good first step before processing it is checking what size it is using `np.shape`, just as we did when analysing functional MRI data in Lab 1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "np.shape(data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Perhaps surprisingly, the song, when read in as a signal, is actually a 1984500-by-2 matrix! Why?\n",
    "\n",
    "The track's runtime is 45 seconds, and with a sampling rate of 44.1 kHz, we expect a total of \n",
    "\n",
    "$$45\\ \\text{seconds} \\cdot \\frac{44100\\ \\text{samples}}{1\\ \\text{second}} = 1984500\\ \\text{samples}$$\n",
    "\n",
    "in our data. That explains the first dimension. Why a two column matrix, though?\n",
    "\n",
    "The reason we have two separate columns of data, rather than a single array of 1984500 samples, is due to the use of [two-channel audio](https://en.wikipedia.org/wiki/Stereophonic_sound). When you listen to music with a pair of headphones, each ear is receiving a separate audio stream, hence the need for two samples at each point in time. The same principle applies to laptops or other sound systems with two speakers. \n",
    "\n",
    "**What this means for us is that each channel (i.e., column of this matrix) should be processed as a distinct, 1D signal.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As a final step before moving on, we'll crop our song to the first 10 seconds. This will make processing go much faster, and we'll still be able to hear what's going on throughout the process."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_sec = 10\n",
    "data_cropped = data[:n_sec * fs, :]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As a sanity check, run the cell below to play the first 10 seconds of the song. The display should show that the file has 10 seconds of audio, and it should sound exactly the same as before."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "wavfile.write(\"wavs/cropped_TakeOnMe.wav\", fs, data_cropped)\n",
    "ipd.Audio(\"wavs/cropped_TakeOnMe.wav\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's make some echoes!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Q1a: Transmission\n",
    "\n",
    "Implement the function `transmit` below to simulate the echo channel. As a reminder, so that you don't have to keep scrolling back and forth, we're modelling the channel as an LTI system with impulse response\n",
    "\n",
    "$$h(n) = \\delta(n) + \\alpha \\delta(n-k)$$\n",
    "\n",
    "where $\\alpha$ (the attentuation factor) and $k$ (the delay of the echo in samples) are provided to you as function arguments, along with the signal to transmit, $x$. Your function should return the result of transmitting $x$ over the channel, performing the parasitic convolution we'll later be trying to undo. \n",
    "\n",
    "**As a reminder, each audio channel should be considered as a distinct signal requiring a separate convolution when transmitting.**\n",
    "\n",
    "#### Quantization\n",
    "\n",
    "All audio we're working with is [quantized](https://en.wikipedia.org/wiki/Quantization) to 16 bits. After processing our signal, we have to renormalize each entry to be a 16-bit integer value, or we'll introduce new distortions to it.\n",
    "\n",
    "After you transmit the song clip **and reassemble it into a two-channel matrix**, say `x_echoed`, apply the following line of code: \n",
    "\n",
    "**`np.int16(x_echoed / np.max(np.abs(x_echoed)) * np.iinfo(np.int16).max)`**. \n",
    "\n",
    "This fits every value to the range [-1, +1] and then rescales it to be within the range of [-32767, 32767]. This will be the last thing you have to do before returning the result.\n",
    "\n",
    "**Hint:** When using Numpy built-in [convolution operation](https://numpy.org/doc/stable/reference/generated/numpy.convolve.html)**`np.convolve`** to perform convolution, all convolutions should be done in \"full\" mode to avoid cutting out data. Since we're using \"full\", there's no need to pad implicit zeros onto the echo channel impulse response; it should only be length $k+1$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def transmit(x, alpha, k):\n",
    "    \"\"\"\n",
    "    Simulate transmission of a two-channel audio signal x over an LTI echo channel which sends \n",
    "    x and a copy of x delayed by k > 0 samples and attenuated by a factor 0 < alpha < 1.\n",
    "    \n",
    "    Parameters:\n",
    "    x        - The audio signal. Assumed to be an Nx2 matrix, where N is the number of audio samples.\n",
    "    alpha    - The attenuation factor. Assumed that 0 < alpha < 1.\n",
    "    k        - The delay factor, in samples. Assumed that k > 0.\n",
    "    \n",
    "    Returns:\n",
    "    x_echoed - The echoed signal.\n",
    "    \"\"\"\n",
    "    # TODO your code here\n",
    "   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once you've finished implementing `transmit`, try it out by running the cell below, which will generate an echo at $80\\%$ the strength of the original signal and with a delay of $2 \\cdot f_s = 88200$ samples (exactly two seconds). \n",
    "\n",
    "**This means our transmitted song will be 12 seconds long.** The original copy starts at time zero and finishes 10 seconds in. The echoed copy starts 2 seconds in, and ends after 12 seconds from the start of the original copy.\n",
    "\n",
    "This cell will take anywhere from several seconds to a minute to run depending on your laptop. Even with 10 seconds of data, we have two $10 \\cdot 44100$ entry convolutions to compute, which will take some time. If it takes longer than a few minutes, your code's probably wrong."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "x_echo = transmit(data_cropped, .8, 2*fs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Run the cell below to play your echo-corrupted song. \n",
    "\n",
    "You should hear a second copy of the track that comes in two seconds later. This means that the first and last two seconds of the audio should only contain one track. The first two seconds will contain the start of the original, and the last two seconds will be the end of the echo. It should be easy to tell if your result is correct or not by just listening."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "wavfile.write(\"wavs/echoed_TakeOnMe.wav\", fs, x_echo)\n",
    "ipd.Audio(\"wavs/echoed_TakeOnMe.wav\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Q1b: Deconvolving\n",
    "\n",
    "Implement the function `deconvolve` below using the Fourier deconvolution algorithm described in the background. Feel free to use NumPy's [fft](https://docs.scipy.org/doc/numpy/reference/generated/numpy.fft.fft.html#numpy.fft.fft) and [inverse fft](https://docs.scipy.org/doc/numpy/reference/generated/numpy.fft.ifft.html#numpy.fft.ifft) functions for this part.\n",
    "\n",
    "As with Lab 3's question on oscilloscope signal alignment, we're going to encounter the issue of numerical noise here yielding an erroneously nonzero imaginary part in our final result. **Be sure to take the real part of any signal returned to avoid a fake imaginary part showing up.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def deconvolve(y, h):\n",
    "    \"\"\"\n",
    "    Perform a Fourier deconvolution to deconvolve h \"out of\" y, assuming\n",
    "    that h, y and the deconvolved signal are both real-valued.\n",
    "    \n",
    "    Parameters:\n",
    "    y - The signal to deconvolve h out of.\n",
    "    h - The impulse response used in the parasitic convolution.\n",
    "    \"\"\"\n",
    "    ## TODO ##\n",
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As a sanity check, let's do a toy example of echo cancellation. \n",
    "\n",
    "We'll set:\n",
    "- $x(n) = .5 \\delta(n) + \\delta(n-1) + .5 \\delta(n-2)$, a three-point pulse.\n",
    "- $h(n) = \\delta(n) + .4 \\delta(n - 7)$, which will generate an echo with a seven-sample delay at $40\\%$ the original's strength.\n",
    "\n",
    "Run the cell below to see what this looks like."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = np.array([.5, 1, .5]); h = np.array([1, 0, 0, 0, 0, 0, 0, .4]); y = np.convolve(x, h, \"full\")\n",
    "\n",
    "x_pad = np.concatenate((x, np.zeros(len(y) - len(x))))\n",
    "h_pad = np.concatenate((h, np.zeros(len(y) - len(h))))\n",
    "\n",
    "plt.figure(figsize=(16, 4))\n",
    "plt.subplot(1, 3, 1); plt.stem(x_pad); plt.title(\"Original signal x\")\n",
    "plt.subplot(1, 3, 2); plt.stem(h_pad, linefmt=\"C1\", markerfmt=\"C1o\"); plt.title(\"Impulse response h\")\n",
    "plt.subplot(1, 3, 3); plt.stem(y, linefmt=\"C2\", markerfmt=\"C2o\"); plt.title(\"Echoed version y = h * x\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We want to remove that second pulse that shows up in $y$. \n",
    "\n",
    "This example is indeed \"toy\" --- the echoed pulse and the original don't temporally overlap at all, so we could just zero out the echo to solve the problem. In the real setup, however, $k$ will be small compared to the signal length and the echo and original will be superimposed. In that case, if we zeroed out the samples, we'd be cutting out data, too, and our song might sound bad or just be missing a few seconds of music altogether. Still, this is a great example to test with, since it'll be obvious if we succeed in killing the echo.\n",
    "\n",
    "Run the cell below, and see if you pass the sanity check. The plots for $x$ and $\\hat{x}$ should be identical (minus trivial differences like color)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_hat = deconvolve(y, h_pad)\n",
    "\n",
    "plt.figure(figsize=(16, 4))\n",
    "\n",
    "plt.subplot(1, 3, 1); plt.stem(x_pad); plt.title(\"Original signal x\")\n",
    "plt.subplot(1, 3, 2); plt.stem(y, linefmt=\"C2\", markerfmt=\"C2o\"); plt.title(\"Echoed version y\")\n",
    "plt.subplot(1, 3, 3); plt.stem(x_hat, linefmt=\"C4\", markerfmt=\"C4o\"); plt.title(\"Recovered version x_hat\")\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Q1c: Echo Removal\n",
    "\n",
    "Implement `cancel_echo` below, which removes an echo of strength $\\alpha$ and sample delay $k$ from the signal `x_echo`. We want `cancel_echo(transmit(x, alpha, k), alpha, k)` to return `x` (possibly with extra zeros on the end, which are harmless) for any valid choices of $\\alpha, k$. \n",
    "\n",
    "Don't forget that:\n",
    "1. We must again renormalize the final output audio matrix to 16-bit integer values the way we did in `transmit`.\n",
    "2. The two audio channels must be treated as separate 1D signals. \n",
    "\n",
    "**Hint:** In `deconvolve`, the FFT vectors we divide must be the same length. This means that unlike in `transmit`, where you only defined the impulse response over $k+1$ indices, you should zero pad it to the same length as `x_echo` before doing the deconvolutions. An example of this is in the Q1b sanity check."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cancel_echo(x_echo, alpha, k):\n",
    "    \"\"\"\n",
    "    Cancel an alpha-strength, k-sample delay echo from a two-channel audio signal x_echo\n",
    "    where k > 0 and 0 < alpha < 1.\n",
    "    \n",
    "    Parameters:\n",
    "    x_echo     - The echo-corrupted audio signal. Assumed to be an Nx2 matrix, where N is the number of audio samples.\n",
    "    alpha      - The attenuation factor. Assumed that 0 < alpha < 1.\n",
    "    k          - The delay factor, in samples. Assumed that k > 0.\n",
    "    \n",
    "    Returns:\n",
    "    x_echoless - The signal with the echo cancelled.\n",
    "    \"\"\"\n",
    "    # TODO\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, run the cell below to see how well your echo cancellation algorithm works! If it's correct:\n",
    "- The audio file will be 12 seconds long.\n",
    "- The first 10 seconds will be the original copy of the song.\n",
    "- The last 2 seconds will be empty, as those audio samples are all zeros. Since we cancelled the echo, which was the only thing present at the end of our echoed recording, there's now no music there. Don't worry about these data-less samples, they're harmless. We could crop them to get the exact same signal if we really wanted to, but it doesn't matter as we don't hear anything."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "x_cleaned = cancel_echo(x_echo, .8, 2*fs)\n",
    "wavfile.write(\"wavs/echoless.wav\", fs, x_cleaned)\n",
    "ipd.Audio(\"wavs/echoless.wav\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once you've successfully removed the echo, move on to Q1d."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Q1d: Noisy Deconvolution\n",
    "\n",
    "We removed an echo from a *clean* audio recording, which is no small feat! However, we haven't accounted for noiseâ€”we've assumed that the only unwanted effect that occurs is due to the echo itself. Fourier deconvolution's main flaw is that it tends to amplify noise, a problem which we'll now explore."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Noise Model\n",
    "\n",
    "We'll assume an *additive* noise model: after the parasitic convolution, there is a *noise signal*, which we'll denote as $z$, that is added to the final signal just before measurement. Below is a block diagram. Note that unlike $x$ and $h$, $z$ is random, although we won't dive too much into that aspect.\n",
    "\n",
    "<img src=\"figs/deconv_noise_model.png\" alt=\"drawing\" style=\"width:500px;\"/>\n",
    "\n",
    "Now, we assume $y(n) = (x * h)(n) + z(n)$, and we again want to recover $x$ given only $y, h$. To get a sense for how a noised, echoed signal differs from a noiseless one, let's add a small amount of white Gaussian noise to `x_echo`, the signal from before."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_echo_noisy = x_echo + 800 * np.random.randn(x_echo.shape[0], x_echo.shape[1])\n",
    "x_echo_noisy = np.int16(x_echo_noisy / np.max(np.abs(x_echo_noisy)) * 32767)\n",
    "\n",
    "wavfile.write(\"wavs/echoed_noised_TakeOnMe.wav\", fs, x_echo_noisy)\n",
    "ipd.Audio(\"wavs/echoed_noised_TakeOnMe.wav\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Q:** How does the noised, echoed version sound (the one we get by running the first cell in this part) compared to the echoed version from before? You should still hear the echo, but there's also white noise that's been added. What does the white noise sound like?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<span style=\"color:blue\">**A:** (TODO)</span>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we'll call the same echo cancellation function from before, and see what happens."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_cleaned_noisy = cancel_echo(x_echo_noisy, .8, 2*fs)\n",
    "wavfile.write(\"wavs/echoless_noisy.wav\", fs, x_cleaned_noisy)\n",
    "ipd.Audio(\"wavs/echoless_noisy.wav\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Q:** You should hear that the echo was removed, just as in the noiseless case. What about the noise, though? Is it louder or softer than before the deconvolution?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<span style=\"color:blue\">**A:** (TODO)</span>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Analyzing Noisy Deconvolution\n",
    "\n",
    "Just as understanding how to deconvolve signals is most easily done in the frequency domain, so is performing an error analysis. We know that \n",
    "\n",
    "$$y(n) = (x * h)(n) + z(n) \\implies Y(\\omega) = X(\\omega)H(\\omega) + Z(\\omega).$$\n",
    "\n",
    "The Fourier deconvolution algorithm returns $\\hat{X}(\\omega)$, an estimate of $X(\\omega)$, which is computed as \n",
    "\n",
    "$$\\hat{X}(\\omega) = \\dfrac{Y(\\omega)}{H(\\omega)} = \\dfrac{X(\\omega)H(\\omega) + Z(\\omega)}{H(\\omega)} = X(\\omega) + \\dfrac{Z(\\omega)}{H(\\omega)},$$\n",
    "\n",
    "so the difference between our estimate, $\\hat{X}$, and the true spectrum, $X$, is\n",
    "\n",
    "$$\\hat{X}(\\omega) - X(\\omega) = \\dfrac{Z(\\omega)}{H(\\omega)}.$$\n",
    "\n",
    "In general, this will be a complex number, which isn't very useful as a way to quantify error. Instead, we can consider the *magnitude* of the error between the estimated spectrum and the true one:\n",
    "\n",
    "$$|\\hat{X}(\\omega) - X(\\omega)| = \\dfrac{|Z(\\omega)|}{|H(\\omega)|}$$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As a final sanity check on your understanding of how Fourier deconvolution performs in the presence of noise, answer the following questions. \n",
    "\n",
    "Assume that for all $\\omega$, $|Z(\\omega)| = \\sigma$, which roughly says that the noise is \"equally strong\" at all frequencies. Truly, $z$ is random, so it's wrong to think of $Z$ as a DTFT in the typical sense, but we'll defer those details to EECS 225A. Our analysis below still gets at the heart of the issue and the tradeoffs involved."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Q:** Suppose $\\sigma = 0$. Can we perfectly recover $X$, assuming that $|H(\\omega)| > 0\\ \\forall \\omega$? Explain."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<span style=\"color:blue\">**A:** (TODO)</span>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Q:** As $\\sigma$ increases, intuitively, is our noise getting stronger or weaker? Is it easier, or more difficult, to recover $X$?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<span style=\"color:blue\">**A:** (TODO)</span>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Q:** The impulse response of our degradation system, which caused the echo, is $h(n) = \\delta(n) + \\alpha \\delta(n -k)$, and so its frequency response is $H(\\omega) = 1 + \\alpha e^{-i\\omega k}$. For concreteness, take $k=1$ and $\\alpha=.8$. \n",
    "\n",
    "Which range of frequencies do you expect the noise amplification to be worst at: high ones, or low ones? Why? (*Hint*: Compute $|H(0)|$ and $|H(\\pi)|$)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<span style=\"color:blue\">**A:** (TODO) </span>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# References\n",
    "\n",
    "[1] *Signal reflection (Wikipedia)*. [Link](https://en.wikipedia.org/wiki/Signal_reflection)  \n",
    "[2] *AT&T Archives: Similarities of Wave Behavior*. [Link](https://www.youtube.com/watch?v=DovunOxlY1k)  \n",
    "[3] *Audio feedback (Wikipedia)*. [Link](https://en.wikipedia.org/wiki/Audio_feedback)  \n",
    "[4] *Dereverberation (Wikipedia)*. [Link](https://en.wikipedia.org/wiki/Dereverberation)  \n",
    "[5] *Stereophonic Acoustic Echo Cancellation: Theory and Implementation*. [Link](http://lup.lub.lu.se/search/ws/files/4596819/1001945.pdf)  \n",
    "[6] *Restoration of Hubble Space Telescope Images and Spectra*. [Link](http://www.stsci.edu/hst/HST_overview/documents/RestorationofHSTImagesandSpectra.pdf)  \n",
    "[7] *Richardson-Lucy deconvolution (Wikipedia)*. [Link](https://en.wikipedia.org/wiki/Richardson%E2%80%93Lucy_deconvolution)  \n",
    "[8] *Wiener deconvolution (Wikipedia)*. [Link](https://en.wikipedia.org/wiki/Wiener_deconvolution)  \n",
    "[9] *Signals, Systems, and Inference, Chapter 11: Wiener Filtering*. [Link](https://ocw.mit.edu/courses/electrical-engineering-and-computer-science/6-011-introduction-to-communication-control-and-signal-processing-spring-2010/readings/MIT6_011S10_chap11.pdf)  \n",
    "[10] 2D convolution GIF. [Link](https://upload.wikimedia.org/wikipedia/commons/4/4f/3D_Convolution_Animation.gif)  \n",
    "[11] *LTI Models and Convolution, Section 11.2.3: Deconvolution*. [Link](https://ocw.mit.edu/courses/electrical-engineering-and-computer-science/6-02-introduction-to-eecs-ii-digital-communication-systems-fall-2012/readings/MIT6_02F12_chap11.pdf)  \n",
    "[12] *The Scientist and Engineer's Guide to Digital Signal Processing: Chapter 17, Custom Filters and Deconvolution*. [Link](https://www.dspguide.com/ch17/2.htm)  \n",
    "[13] *Statistics of natural image categories*. [Link](http://web.mit.edu/torralba/www/ne3302.pdf)"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
